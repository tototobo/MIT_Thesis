{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# PRIOR TO THIS SCRIPT: \n",
    "# 1) Verify that the price is given in $/MMBtu. \n",
    "# 2) Verify that the price for 1/1/2021 is given. If not, add it manually (take the most recent price available)\n",
    "# DO NOT FORGET TO ACCOUNT FOR THE TIME CHANGE (DST) IN MARCH AND NOVEMBER: done on lines 27-35\n",
    "# For 2020, delete the hour 3-4am on 3/8/2020 and add the hour 2-3am on 11/1/2020\n",
    "#######################################################################################################################\n",
    "\n",
    "# taking datas from gas and adding missing dates (weekends) + hours\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the original CSV file\n",
    "df = pd.read_csv(\"SPGlobal_CommodityCharting(2012-2021)_05-Jun-2023.csv\")\n",
    "\n",
    "# rename the column \"Closing Price\" to \"NG\"\n",
    "df = df.rename(columns={\"Closing Price\": \"NG\"})\n",
    "\n",
    "# Convert the \"Date\" column to a datetime object\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "# Create a new DataFrame with all dates from 01/01/2021 to 12/31/2021\n",
    "full_date_range = pd.date_range(start=\"01/01/2012\", end=\"01/01/2022\", freq=\"H\")\n",
    "full_df = pd.DataFrame({\"Date\": full_date_range})\n",
    "\n",
    "# Merge the two DataFrames based on the \"Date\" column, filling in missing values with the previous day's closing price\n",
    "merged_df = full_df.merge(df, on=\"Date\", how=\"left\").fillna(method=\"ffill\")\n",
    "\n",
    "#delete the last row (01/01/2022)\n",
    "merged_df = merged_df[:-1]\n",
    "\n",
    "# Delete the row with the date 2021-03-14 03:00:00, which is the hour that is skipped due to Daylight Savings Time. Do the same for all the years up to 2012\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2021-03-14 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2020-03-08 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2019-03-10 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2018-03-11 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2017-03-12 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2016-03-13 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2015-03-08 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2014-03-09 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2013-03-10 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2012-03-11 03:00:00'].index)\n",
    "\n",
    "# Duplicate the row with the date 2021-11-07 02:00:00 using concat, which is the hour that is duplicated due to Daylight Savings Time. Do the same for all the years up to 2012\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2021-11-07 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2020-11-01 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2019-11-03 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2018-11-04 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2017-11-05 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2016-11-06 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2015-11-01 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2014-11-02 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2013-11-03 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2012-11-04 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "\n",
    "# Sort the DataFrame by the \"Date\" column, ascending\n",
    "merged_df = merged_df.sort_values(by=\"Date\", ascending=True)\n",
    "\n",
    "# add the column \"Time_Index\" ranging from 1 to len(merged_df)\n",
    "merged_df['Time_Index'] = range(1, len(merged_df) + 1)\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"NG(HH)2012-2021.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# PRIOR TO THIS SCRIPT: \n",
    "# 1) Verify that the price is given in $/MMBtu. Coal is usually $/tonne (divide by 27.78 to convert to $/MMBtu)\n",
    "# 2) Verify that the price for 1/1/2021 is given. If not, add it manually (take the most recent price available)\n",
    "# DO NOT FORGET TO ACCOUNT FOR THE TIME CHANGE (DST) IN MARCH AND NOVEMBER: done on lines 27-35\n",
    "# For 2021, delete the hour 3-4am on 3/14/2021 and add the hour 2-3am on 11/7/2021 \n",
    "#######################################################################################################################\n",
    "\n",
    "# taking datas from coal and adding missing dates (weekends) + hours\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the original CSV file\n",
    "df = pd.read_csv(\"Coal_12_31_21-01_03_12.csv\")\n",
    "\n",
    "# rename the column \"Closing price\" to \"coal\"\n",
    "df = df.rename(columns={\"Closing price\": \"coal\"})\n",
    "\n",
    "# Convert the \"Date\" column to a datetime object\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "# Create a new DataFrame with all dates from 01/01/2021 to 12/31/2021\n",
    "full_date_range = pd.date_range(start=\"01/01/2012\", end=\"01/01/2022\", freq=\"H\")\n",
    "full_df = pd.DataFrame({\"Date\": full_date_range})\n",
    "\n",
    "# Merge the two DataFrames based on the \"Date\" column, filling in missing values with the previous day's closing price\n",
    "merged_df = full_df.merge(df, on=\"Date\", how=\"left\").fillna(method=\"ffill\")\n",
    "\n",
    "#delete the last row (01/01/2022)\n",
    "merged_df = merged_df[:-1]\n",
    "\n",
    "# Delete the row with the date 2021-03-14 03:00:00, which is the hour that is skipped due to Daylight Savings Time. Do the same for all the years up to 2012\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2021-03-14 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2020-03-08 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2019-03-10 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2018-03-11 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2017-03-12 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2016-03-13 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2015-03-08 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2014-03-09 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2013-03-10 03:00:00'].index)\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2012-03-11 03:00:00'].index)\n",
    "\n",
    "# Duplicate the row with the date 2021-11-07 02:00:00 using concat, which is the hour that is duplicated due to Daylight Savings Time. Do the same for all the years up to 2012\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2021-11-07 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2020-11-01 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2019-11-03 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2018-11-04 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2017-11-05 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2016-11-06 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2015-11-01 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2014-11-02 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2013-11-03 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2012-11-04 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "\n",
    "# Sort the DataFrame by the \"Date\" column, ascending\n",
    "merged_df = merged_df.sort_values(by=\"Date\", ascending=True)\n",
    "\n",
    "# add the column \"Time_Index\" ranging from 1 to len(merged_df)\n",
    "merged_df['Time_Index'] = range(1, len(merged_df) + 1)\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"Coal2012-2021.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n",
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n",
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n",
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n",
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n",
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n",
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n",
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        NG  Time_Index      coal                 Date  None   uranium\n",
      "0  0.05306         0.0  0.095488                  0.0   0.0  0.000000\n",
      "1  2.98900         1.0  3.921886  2012-01-01 00:00:00   0.0  0.705536\n",
      "2  2.98900         2.0  3.921886  2012-01-01 01:00:00   0.0  0.705536\n",
      "3  2.98900         3.0  3.921886  2012-01-01 02:00:00   0.0  0.705536\n",
      "4  2.98900         4.0  3.921886  2012-01-01 03:00:00   0.0  0.705536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n",
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_8684\\2961826406.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################################################\n",
    "# Build the 'Fuel_Data20xx.csv' files\n",
    "#######################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the NG(HH) file\n",
    "ng_df = pd.read_csv(\"NG(HH)2012-2021.csv\")\n",
    "\n",
    "# Read the Coal file\n",
    "coal_df = pd.read_csv(\"Coal2012-2021.csv\")\n",
    "\n",
    "# Merge the two DataFrames based on the \"Time_Index\" column\n",
    "merged_df = ng_df.merge(coal_df, on=\"Time_Index\", how=\"left\")\n",
    "merged_df[\"Date\"] = coal_df[\"Date\"]\n",
    "\n",
    "# Delete the \"Date_x\" and \"Date_y\" columns\n",
    "merged_df = merged_df.drop(columns=[\"Date_x\", \"Date_y\"])\n",
    "\n",
    "# add a column \"None\" with values 0, a column \"uranium\" with values 0.705536\n",
    "merged_df['None'] = 0\n",
    "merged_df['uranium'] = 0.705536\n",
    "\n",
    "# year in 'Date' column goes from 2012 to 2021. In a loop, select the corresponding years and write the merged DataFrame to a new CSV file for each year, named 'Fuel_Data20xx.csv'\n",
    "for year in range(2012, 2022):\n",
    "    year_df = merged_df[merged_df['Date'].str.contains(str(year))]\n",
    "    year_df.loc[-1] = [0.05306, 0, 0.095488, 0, 0, 0]\n",
    "    year_df.index = year_df.index + 1\n",
    "    year_df = year_df.sort_index()\n",
    "    year_df['Time_Index'] = range(0, len(year_df))\n",
    "    year_df.to_csv(\"Fuel_Data\" + str(year) + \".csv\", index=False)\n",
    "\n",
    "# Print the merged DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2012prices.csv\n",
      "Saved 2013prices.csv\n",
      "Saved 2014prices.csv\n",
      "Saved 2015prices.csv\n",
      "Saved 2016prices.csv\n",
      "Saved 2017prices.csv\n",
      "Saved 2018prices.csv\n",
      "Saved 2019prices.csv\n",
      "Saved 2020prices.csv\n",
      "Saved 2021prices.csv\n"
     ]
    }
   ],
   "source": [
    "# Modify the price file to only keep the average bus price.\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get a list of file paths for files matching the pattern \"20xxprices.xlsx\"\n",
    "file_paths = glob.glob(\"20??prices.xlsx\")\n",
    "\n",
    "# Iterate over the file paths\n",
    "for file_path in file_paths:\n",
    "    # Extract the year from the file name\n",
    "    year = file_path.split(\"prices\")[0][-4:]\n",
    "\n",
    "    # Read in the Excel file\n",
    "    excel_file = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "    # Loop through each sheet, filter out non-\"HB_BUSAVG\" rows, and concatenate the results\n",
    "    concatenated_df = pd.concat(\n",
    "        [df.loc[df[\"Settlement Point\"] == \"HB_BUSAVG\"] for df in excel_file.values()]\n",
    "    )\n",
    "\n",
    "    # Drop the \"Settlement Point\", \"Repeated Hour Flag\", and rename the \"Settlement Point Price\" column to \"Price\", and add a column \"Time_Index\" ranging from 1 to len(concatenated_df)\n",
    "    concatenated_df = concatenated_df.drop(columns=[\"Settlement Point\", \"Repeated Hour Flag\"])\n",
    "    concatenated_df = concatenated_df.rename(columns={\"Settlement Point Price\": \"Price\"})\n",
    "    concatenated_df['Time_Index'] = range(1, len(concatenated_df) + 1)\n",
    "\n",
    "    # Construct the output file path\n",
    "    csv_file_path = f\"{year}prices.csv\"\n",
    "\n",
    "    # Write the concatenated dataframe to a CSV file\n",
    "    concatenated_df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Saved {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# Modify the load file to only keep the ERCOT load.\n",
    "##############################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Loop through the years 2012-2021\n",
    "for year in range(2012, 2022):\n",
    "    # Construct the file name based on the year\n",
    "    file_name = f\"Native_Load_{year}.xlsx\"\n",
    "    \n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(file_name)\n",
    "    \n",
    "    # Delete the columns that does not contain \"ERCOT\" or \"Hour\" in the column name\n",
    "    df = df.loc[:, df.columns.str.contains('ERCOT')]\n",
    "\n",
    "    # Rename the \"Load_MW_z1\" column to \"ERCOT\" and put it in the first column\n",
    "    df.rename(columns={'ERCOT': 'Load_MW_z1'}, inplace=True)\n",
    "\n",
    "    # add columns 'Unnamed: 0.1.1.1.1.1', 'Voll', 'Demand_Segment', 'Cost_of_Demand_Curtailment_per_MW', 'Max_Demand_Curtailment', 'Rep_Periods', 'Timesteps_per_Rep_Period', 'Sub_Weights', 'Time_Index' before the first column\n",
    "    df.insert(0, 'Unnamed: 0.1.1.1.1.1', \"\")\n",
    "    df.insert(1, 'Voll', \"\")\n",
    "    df.insert(2, 'Demand_Segment', \"\")\n",
    "    df.insert(3, 'Cost_of_Demand_Curtailment_per_MW', \"\")\n",
    "    df.insert(4, 'Max_Demand_Curtailment', \"\")\n",
    "    df.insert(5, 'Rep_Periods', \"\")\n",
    "    df.insert(6, 'Timesteps_per_Rep_Period', \"\")\n",
    "    df.insert(7, 'Sub_Weights', \"\")\n",
    "    df.insert(8, 'Time_Index', \"\")  \n",
    "\n",
    "    # Time_Index ranges from 1 to length of df\n",
    "    df['Time_Index'] = range(1, len(df) + 1)\n",
    "    df['Unnamed: 0.1.1.1.1.1'] = range(0, len(df))\n",
    "    \n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    new_file_name = f\"Load_data{year}.csv\"\n",
    "    df.to_csv(new_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2016-12-01\n",
      "1    2016-12-01\n",
      "2    2016-12-01\n",
      "3    2016-12-01\n",
      "4    2016-12-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-11-01\n",
      "1    2016-11-01\n",
      "2    2016-11-01\n",
      "3    2016-11-01\n",
      "4    2016-11-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-10-01\n",
      "1    2016-10-01\n",
      "2    2016-10-01\n",
      "3    2016-10-01\n",
      "4    2016-10-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-09-01\n",
      "1    2016-09-01\n",
      "2    2016-09-01\n",
      "3    2016-09-01\n",
      "4    2016-09-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-08-01\n",
      "1    2016-08-01\n",
      "2    2016-08-01\n",
      "3    2016-08-01\n",
      "4    2016-08-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-07-01\n",
      "1    2016-07-01\n",
      "2    2016-07-01\n",
      "3    2016-07-01\n",
      "4    2016-07-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-06-01\n",
      "1    2016-06-01\n",
      "2    2016-06-01\n",
      "3    2016-06-01\n",
      "4    2016-06-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-05-01\n",
      "1    2016-05-01\n",
      "2    2016-05-01\n",
      "3    2016-05-01\n",
      "4    2016-05-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-04-01\n",
      "1    2016-04-01\n",
      "2    2016-04-01\n",
      "3    2016-04-01\n",
      "4    2016-04-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-03-01\n",
      "1    2016-03-01\n",
      "2    2016-03-01\n",
      "3    2016-03-01\n",
      "4    2016-03-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-02-01\n",
      "1    2016-02-01\n",
      "2    2016-02-01\n",
      "3    2016-02-01\n",
      "4    2016-02-01\n",
      "Name: Date, dtype: object\n",
      "0    2016-01-01\n",
      "1    2016-01-01\n",
      "2    2016-01-01\n",
      "3    2016-01-01\n",
      "4    2016-01-01\n",
      "Name: Date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "excel_file = 'IntGenbyFuel2016.xlsx'\n",
    "\n",
    "# Read all sheet names from the Excel file\n",
    "all_sheet_names = pd.ExcelFile(excel_file).sheet_names\n",
    "\n",
    "# Filter the sheet names that contain the strings 'Jan', 'Feb', ..., 'Dec'\n",
    "filtered_sheet_names = [sheet_name for sheet_name in all_sheet_names if any(month in sheet_name for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])]\n",
    "\n",
    "# Create an empty list to store DataFrames for each sheet\n",
    "dfs = []\n",
    "\n",
    "# Read each sheet and append it to the dfs list\n",
    "for sheet_name in filtered_sheet_names:\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "\n",
    "    # Convert column names to strings\n",
    "    df.columns = df.columns.astype(str)\n",
    "\n",
    "    # Split the 'Date-Fuel' column into 'Date' and 'Fuel' if there is a column named 'Date-Fuel'\n",
    "    if 'Date-Fuel' in df.columns:\n",
    "        df[['Date', 'Fuel']] = df['Date-Fuel'].str.split('_|-', n=1).apply(pd.Series)\n",
    "        df = df.drop(columns=['Date-Fuel'])\n",
    "\n",
    "    #Replace the rows that contain 'Gas_CC' with 'Gas-CC'\n",
    "    df['Fuel'] = df['Fuel'].str.replace('Gas_CC', 'Gas-CC')\n",
    "    df['Fuel'] = df['Fuel'].str.replace('Sun', 'Solar')\n",
    "\n",
    "    # Remove seconds from column names\n",
    "    df.columns = [col.rsplit(':', 1)[0] if col.count(':') > 1 else col for col in df.columns]\n",
    "\n",
    "    # Remove the leading zero from column names\n",
    "    df.columns = [col[1:] if col.startswith('0') and col[1:2].isdigit() and 'DST' not in col else col for col in df.columns]\n",
    "\n",
    "    # Append the modified DataFrame to the dfs list\n",
    "    dfs.append(df)\n",
    "\n",
    "# # convert the 'Date' column to datetime without hour, just date\n",
    "# df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "\n",
    "# print the date column of all the DataFrame in the dfs list\n",
    "for df in dfs:\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "    print(df['Date'].head())\n",
    "\n",
    "# Save the modified DataFrames to a new Excel file\n",
    "with pd.ExcelWriter(excel_file) as writer:\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.to_excel(writer, sheet_name=filtered_sheet_names[i], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuel                        Biomass         Coal          Gas        Gas-CC   \n",
      "0    2016-01-01 01:00:00  68.607081  8867.420717  1263.751263  13985.084000  \\\n",
      "1    2016-01-01 02:00:00  67.624264  8701.534820  1232.198232  13745.089105   \n",
      "2    2016-01-01 03:00:00  67.264939  8564.574727  1225.805034  13557.469356   \n",
      "3    2016-01-01 04:00:00  67.323732  8451.511363  1219.869540  13520.514922   \n",
      "4    2016-01-01 05:00:00  67.356138  8622.309826  1220.422316  13711.908162   \n",
      "\n",
      "Fuel      Hydro      Nuclear     Other  Solar         Wind  \n",
      "0     60.829692  5116.751697  3.107936    0.0  4539.804434  \n",
      "1     61.814918  5116.183184  2.883249    0.0  4559.565857  \n",
      "2     58.682427  5116.615117  2.881236    0.0  4558.567919  \n",
      "3     55.783400  5116.412792  3.033120    0.0  4672.114592  \n",
      "4     56.714066  5114.987089  3.147610    0.0  4699.275775  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_17260\\871658929.py:110: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  combined_data = combined_data.drop(columns=['SortKey', 'Date'])\n"
     ]
    }
   ],
   "source": [
    "#Convert the IntGenByFuelType file to a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "excel_file = 'IntGenbyFuel2016.xlsx'\n",
    "\n",
    "# Read all sheet names from the Excel file\n",
    "all_sheet_names = pd.ExcelFile(excel_file).sheet_names\n",
    "\n",
    "# Filter the sheet names that contain the strings 'Jan', 'Feb', ..., 'Dec'\n",
    "filtered_sheet_names = [sheet_name for sheet_name in all_sheet_names if any(month in sheet_name for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])]\n",
    "\n",
    "# Create an empty list to store DataFrames for each sheet\n",
    "dfs = []\n",
    "\n",
    "# Read each sheet and append it to the dfs list\n",
    "for sheet_name in filtered_sheet_names:\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the DataFrames in the dfs list\n",
    "combined_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# #drop the column \"Settlement Type\"\n",
    "# combined_data = combined_data.drop(columns=['Settlement Type', 'Total'])\n",
    "\n",
    "# add columns for each hour of the day, ranging from 1 to 24. Values are the sum of each quarter hour\n",
    "combined_data['1'] = combined_data['0:15'] + combined_data['0:30'] + combined_data['0:45'] + combined_data['1:00']\n",
    "combined_data['2'] = combined_data['1:15'] + combined_data['1:30'] + combined_data['1:45'] + combined_data['2:00']\n",
    "combined_data['3'] = combined_data['2:15'] + combined_data['2:30'] + combined_data['2:45'] + combined_data['3:00']\n",
    "combined_data['4'] = combined_data['3:15'] + combined_data['3:30'] + combined_data['3:45'] + combined_data['4:00']\n",
    "combined_data['5'] = combined_data['4:15'] + combined_data['4:30'] + combined_data['4:45'] + combined_data['5:00']\n",
    "combined_data['6'] = combined_data['5:15'] + combined_data['5:30'] + combined_data['5:45'] + combined_data['6:00']\n",
    "combined_data['7'] = combined_data['6:15'] + combined_data['6:30'] + combined_data['6:45'] + combined_data['7:00']\n",
    "combined_data['8'] = combined_data['7:15'] + combined_data['7:30'] + combined_data['7:45'] + combined_data['8:00']\n",
    "combined_data['9'] = combined_data['8:15'] + combined_data['8:30'] + combined_data['8:45'] + combined_data['9:00']\n",
    "combined_data['10'] = combined_data['9:15'] + combined_data['9:30'] + combined_data['9:45'] + combined_data['10:00']\n",
    "combined_data['11'] = combined_data['10:15'] + combined_data['10:30'] + combined_data['10:45'] + combined_data['11:00']\n",
    "combined_data['12'] = combined_data['11:15'] + combined_data['11:30'] + combined_data['11:45'] + combined_data['12:00']\n",
    "combined_data['13'] = combined_data['12:15'] + combined_data['12:30'] + combined_data['12:45'] + combined_data['13:00']\n",
    "combined_data['14'] = combined_data['13:15'] + combined_data['13:30'] + combined_data['13:45'] + combined_data['14:00']\n",
    "combined_data['15'] = combined_data['14:15'] + combined_data['14:30'] + combined_data['14:45'] + combined_data['15:00']\n",
    "combined_data['16'] = combined_data['15:15'] + combined_data['15:30'] + combined_data['15:45'] + combined_data['16:00']\n",
    "combined_data['17'] = combined_data['16:15'] + combined_data['16:30'] + combined_data['16:45'] + combined_data['17:00']\n",
    "combined_data['18'] = combined_data['17:15'] + combined_data['17:30'] + combined_data['17:45'] + combined_data['18:00']\n",
    "combined_data['19'] = combined_data['18:15'] + combined_data['18:30'] + combined_data['18:45'] + combined_data['19:00']\n",
    "combined_data['20'] = combined_data['19:15'] + combined_data['19:30'] + combined_data['19:45'] + combined_data['20:00']\n",
    "combined_data['21'] = combined_data['20:15'] + combined_data['20:30'] + combined_data['20:45'] + combined_data['21:00']\n",
    "combined_data['22'] = combined_data['21:15'] + combined_data['21:30'] + combined_data['21:45'] + combined_data['22:00']\n",
    "combined_data['23'] = combined_data['22:15'] + combined_data['22:30'] + combined_data['22:45'] + combined_data['23:00']\n",
    "combined_data['0'] = combined_data['23:15'] + combined_data['23:30'] + combined_data['23:45'] + combined_data['0:00']\n",
    "combined_data['2 (DST)'] = combined_data['01:15 (DST)'] + combined_data['01:30 (DST)'] + combined_data['01:45 (DST)'] + combined_data['02:00 (DST)']\n",
    "\n",
    "# keep only the fuel and date columns, and the column we just created\n",
    "combined_data = combined_data[['Fuel', 'Date', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
    "                                    '21', '22', '23', '0', '2 (DST)']]\n",
    "\n",
    "# instead of having a column for each hour, we want to have a column for the date and hour and a column for the value\n",
    "# first, we need to melt the data\n",
    "combined_data = pd.melt(combined_data, id_vars=['Fuel', 'Date'], var_name='Hour', value_name='Value')\n",
    "\n",
    "# now we need to combine the date and hour columns\n",
    "combined_data['Date'] = combined_data['Date'].astype(str)\n",
    "combined_data['Hour'] = combined_data['Hour'].astype(str)\n",
    "combined_data['Hour'] = combined_data['Hour'].str.zfill(2)\n",
    "\n",
    "# we need to convert the hour column to datetime\n",
    "\n",
    "combined_data['DateHour'] = combined_data['Date'] + ' ' + combined_data['Hour']\n",
    "\n",
    "# now we can drop the date and hour columns\n",
    "combined_data = combined_data.drop(columns=['Hour'])\n",
    "combined_data = combined_data.dropna(subset=['Value'])\n",
    "\n",
    "# save the row with the (DST) value in a separate dataframe and remove it from the main dataframe\n",
    "dst_data = combined_data[combined_data['DateHour'].str.contains('DST')]\n",
    "combined_data = combined_data[~combined_data['DateHour'].str.contains('DST')]\n",
    "\n",
    "# remove the (DST) from the DateHour column\n",
    "dst_data['DateHour'] = dst_data['DateHour'].str.replace(' (DST)', '')\n",
    "\n",
    "# now we can convert the DateHour column to a datetime\n",
    "combined_data['DateHour'] = pd.to_datetime(combined_data['DateHour'], format='%Y-%m-%d %H')\n",
    "dst_data['DateHour'] = pd.to_datetime(dst_data['DateHour'], format='%Y-%m-%d %H')\n",
    "\n",
    "# Pivot the DataFrame to have 'Fuel' as columns\n",
    "combined_data = combined_data.pivot_table(index=['DateHour', 'Date'], columns=['Fuel'], values=['Value']).reset_index()\n",
    "dst_data = dst_data.pivot_table(index=['DateHour', 'Date'], columns=['Fuel'], values=['Value']).reset_index()\n",
    "\n",
    "# remove the hour change row in march\n",
    "combined_data = combined_data[combined_data[('Value', 'Biomass')] != 0]\n",
    "\n",
    "# add the (DST) data back to the main dataframe using concat\n",
    "combined_data = pd.concat([combined_data, dst_data])\n",
    "\n",
    "# Create a custom sorting key to sort the 'DateHour' column\n",
    "def custom_sort_key(x):\n",
    "    hour = x.hour\n",
    "    if hour == 0:\n",
    "        hour = 24  # Assign a value greater than other hours to make it appear last\n",
    "    return hour\n",
    "\n",
    "combined_data['SortKey'] = combined_data['DateHour'].apply(custom_sort_key)\n",
    "\n",
    "# Sort the DataFrame by the custom sorting key\n",
    "combined_data = combined_data.sort_values(by=['Date', 'SortKey'])\n",
    "\n",
    "# Remove the 'SortKey' and 'Date' column\n",
    "combined_data = combined_data.drop(columns=['SortKey', 'Date'])\n",
    "\n",
    "# Keep only the fuel names in the column names\n",
    "combined_data.columns = combined_data.columns.droplevel(0)\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "combined_data = combined_data.reset_index(drop=True)\n",
    "\n",
    "print(combined_data.head(5))\n",
    "\n",
    "# Save the combined data to a CSV file  \n",
    "file_name = excel_file.split('.')[0]\n",
    "new_file_name = f\"{file_name}_hourly.csv\"\n",
    "combined_data.to_csv(new_file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

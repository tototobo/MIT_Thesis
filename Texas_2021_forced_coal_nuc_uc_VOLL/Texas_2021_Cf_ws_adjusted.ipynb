{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# PRIOR TO THIS SCRIPT: \n",
    "# 1) Verify that the price is given in $/MMBtu. \n",
    "# 2) Verify that the price for 1/1/2021 is given. If not, add it manually (take the most recent price available)\n",
    "# DO NOT FORGET TO ACCOUNT FOR THE TIME CHANGE (DST) IN MARCH AND NOVEMBER: done on lines 27-35\n",
    "# For 2021, delete the hour 3-4am on 3/14/2021 and add the hour 2-3am on 11/7/2021 \n",
    "#######################################################################################################################\n",
    "\n",
    "# taking datas from gas and adding missing dates (weekends) + hours\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the original CSV file\n",
    "df = pd.read_csv(\"SPGlobal_CommodityCharting(Chart)_23-May-2023.csv\")\n",
    "\n",
    "# Convert the \"Date\" column to a datetime object\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "# Create a new DataFrame with all dates from 01/01/2021 to 12/31/2021\n",
    "full_date_range = pd.date_range(start=\"01/01/2021\", end=\"01/01/2022\", freq=\"H\")\n",
    "full_df = pd.DataFrame({\"Date\": full_date_range})\n",
    "\n",
    "# Merge the two DataFrames based on the \"Date\" column, filling in missing values with the previous day's closing price\n",
    "merged_df = full_df.merge(df, on=\"Date\", how=\"left\").fillna(method=\"ffill\")\n",
    "\n",
    "#delete the last row (01/01/2022)\n",
    "merged_df = merged_df[:-1]\n",
    "\n",
    "# Delete the row with the date 2021-03-14 03:00:00\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2021-03-14 03:00:00'].index)\n",
    "\n",
    "# Duplicate the row with the date 2021-11-07 02:00:00 using concat\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2021-11-07 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "\n",
    "# Sort the DataFrame by the \"Date\" column, ascending\n",
    "merged_df = merged_df.sort_values(by=\"Date\", ascending=True)\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"NG(HH)2021.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# PRIOR TO THIS SCRIPT: \n",
    "# 1) Verify that the price is given in $/MMBtu. Coal is usually $/tonne\n",
    "# 2) Verify that the price for 1/1/2021 is given. If not, add it manually (take the most recent price available)\n",
    "# DO NOT FORGET TO ACCOUNT FOR THE TIME CHANGE (DST) IN MARCH AND NOVEMBER: done on lines 27-35\n",
    "# For 2021, delete the hour 3-4am on 3/14/2021 and add the hour 2-3am on 11/7/2021 \n",
    "#######################################################################################################################\n",
    "\n",
    "# taking datas from coal and adding missing dates (weekends) + hours\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the original CSV file\n",
    "df = pd.read_csv(\"Coal_12_31_21-12_31_20.csv\")\n",
    "\n",
    "# Convert the \"Date\" column to a datetime object\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "# Create a new DataFrame with all dates from 01/01/2021 to 12/31/2021\n",
    "full_date_range = pd.date_range(start=\"01/01/2021\", end=\"01/01/2022\", freq=\"H\")\n",
    "full_df = pd.DataFrame({\"Date\": full_date_range})\n",
    "\n",
    "# Merge the two DataFrames based on the \"Date\" column, filling in missing values with the previous day's closing price\n",
    "merged_df = full_df.merge(df, on=\"Date\", how=\"left\").fillna(method=\"ffill\")\n",
    "\n",
    "#delete the last row (01/01/2022)\n",
    "merged_df = merged_df[:-1]\n",
    "\n",
    "# Delete the row with the date 2021-03-14 03:00:00\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Date'] == '2021-03-14 03:00:00'].index)\n",
    "\n",
    "# Duplicate the row with the date 2021-11-07 02:00:00 using concat\n",
    "row_to_duplicate = merged_df[merged_df['Date'] == '2021-11-07 02:00:00']\n",
    "merged_df = pd.concat([merged_df, row_to_duplicate])\n",
    "\n",
    "# Sort the DataFrame by the \"Date\" column, ascending\n",
    "merged_df = merged_df.sort_values(by=\"Date\", ascending=True)\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"Coal2021.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the price file to only keep the average bus price.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the Excel file\n",
    "excel_file = pd.read_excel(\"2021prices.xlsx\", sheet_name=None)\n",
    "\n",
    "# Loop through each sheet, filter out non-\"HB_BUSAVG\" rows, and concatenate the results\n",
    "concatenated_df = pd.concat(\n",
    "    [df.loc[df[\"Settlement Point\"] == \"HB_BUSAVG\"] for df in excel_file.values()]\n",
    ")\n",
    "\n",
    "# Write the concatenated dataframe to a CSV file\n",
    "concatenated_df.to_csv(\"2021prices.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuel                        Biomass         Coal          Gas        Gas-CC   \n",
      "0    2021-01-01 01:00:00  29.641907  6226.403543  1264.612197  16947.276551  \\\n",
      "1    2021-01-01 02:00:00  29.653193  6166.239724  1336.967578  16334.880788   \n",
      "2    2021-01-01 03:00:00  29.666134  6103.444333  1369.308736  16105.976972   \n",
      "3    2021-01-01 04:00:00  29.670460  6236.221736  1370.516529  16917.627213   \n",
      "4    2021-01-01 05:00:00  29.663568  6819.574839  1343.463393  17740.373602   \n",
      "5    2021-01-01 06:00:00  28.529615  7703.488911  1499.707446  18377.752842   \n",
      "6    2021-01-01 07:00:00  29.660938  8404.042171  1661.747781  19054.751362   \n",
      "7    2021-01-01 08:00:00  29.664135  8864.096263  1673.959457  19600.166075   \n",
      "8    2021-01-01 09:00:00  29.667813  8946.533163  1675.453231  19434.595076   \n",
      "9    2021-01-01 10:00:00  29.627579  7976.463748  1650.879532  18961.349828   \n",
      "\n",
      "Fuel      Hydro      Nuclear     Other        Solar          Wind  \n",
      "0     35.600353  5103.196339  2.871013     0.000512  13984.762021  \n",
      "1     12.210071  5102.015677  4.068958     0.000995  14289.144771  \n",
      "2     11.458139  5103.524853  3.391051     0.001144  14221.109204  \n",
      "3     11.462853  5104.190140  4.735174     0.000443  13246.113229  \n",
      "4     11.449410  5104.197354  4.634495     4.195011  12231.338868  \n",
      "5     10.629999  5102.930427  1.454998     0.001308  11368.335056  \n",
      "6     34.918974  5101.973252  1.665987     0.000503  10737.304313  \n",
      "7     34.575940  5102.027373  2.380326     7.033288  10332.768918  \n",
      "8     34.572316  5102.529790 -0.136572   673.993299   9892.278593  \n",
      "9     34.918011  5103.153579 -7.990801  2069.941880  10063.450372  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsbon\\AppData\\Local\\Temp\\ipykernel_21180\\2158569878.py:104: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  combined_data = combined_data.drop(columns=['SortKey', 'Date'])\n"
     ]
    }
   ],
   "source": [
    "#Convert the IntGenByFuelType file to a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "excel_file = 'IntGenbyFuel2021.xlsx'\n",
    "\n",
    "# List of sheet names in the Excel file\n",
    "sheet_names = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "\n",
    "# Create an empty list to store DataFrames for each sheet\n",
    "dfs = []\n",
    "\n",
    "# Read each sheet and append it to the dfs list\n",
    "for sheet_name in sheet_names:\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the DataFrames in the dfs list\n",
    "combined_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "#drop the column \"Settlement Type\"\n",
    "combined_data = combined_data.drop(columns=['Settlement Type', 'Total'])\n",
    "\n",
    "# add columns for each hour of the day, ranging from 1 to 24. Values are the sum of each quarter hour\n",
    "combined_data['1'] = combined_data['0:15'] + combined_data['0:30'] + combined_data['0:45'] + combined_data['1:00']\n",
    "combined_data['2'] = combined_data['1:15'] + combined_data['1:30'] + combined_data['1:45'] + combined_data['2:00']\n",
    "combined_data['3'] = combined_data['2:15'] + combined_data['2:30'] + combined_data['2:45'] + combined_data['3:00']\n",
    "combined_data['4'] = combined_data['3:15'] + combined_data['3:30'] + combined_data['3:45'] + combined_data['4:00']\n",
    "combined_data['5'] = combined_data['4:15'] + combined_data['4:30'] + combined_data['4:45'] + combined_data['5:00']\n",
    "combined_data['6'] = combined_data['5:15'] + combined_data['5:30'] + combined_data['5:45'] + combined_data['6:00']\n",
    "combined_data['7'] = combined_data['6:15'] + combined_data['6:30'] + combined_data['6:45'] + combined_data['7:00']\n",
    "combined_data['8'] = combined_data['7:15'] + combined_data['7:30'] + combined_data['7:45'] + combined_data['8:00']\n",
    "combined_data['9'] = combined_data['8:15'] + combined_data['8:30'] + combined_data['8:45'] + combined_data['9:00']\n",
    "combined_data['10'] = combined_data['9:15'] + combined_data['9:30'] + combined_data['9:45'] + combined_data['10:00']\n",
    "combined_data['11'] = combined_data['10:15'] + combined_data['10:30'] + combined_data['10:45'] + combined_data['11:00']\n",
    "combined_data['12'] = combined_data['11:15'] + combined_data['11:30'] + combined_data['11:45'] + combined_data['12:00']\n",
    "combined_data['13'] = combined_data['12:15'] + combined_data['12:30'] + combined_data['12:45'] + combined_data['13:00']\n",
    "combined_data['14'] = combined_data['13:15'] + combined_data['13:30'] + combined_data['13:45'] + combined_data['14:00']\n",
    "combined_data['15'] = combined_data['14:15'] + combined_data['14:30'] + combined_data['14:45'] + combined_data['15:00']\n",
    "combined_data['16'] = combined_data['15:15'] + combined_data['15:30'] + combined_data['15:45'] + combined_data['16:00']\n",
    "combined_data['17'] = combined_data['16:15'] + combined_data['16:30'] + combined_data['16:45'] + combined_data['17:00']\n",
    "combined_data['18'] = combined_data['17:15'] + combined_data['17:30'] + combined_data['17:45'] + combined_data['18:00']\n",
    "combined_data['19'] = combined_data['18:15'] + combined_data['18:30'] + combined_data['18:45'] + combined_data['19:00']\n",
    "combined_data['20'] = combined_data['19:15'] + combined_data['19:30'] + combined_data['19:45'] + combined_data['20:00']\n",
    "combined_data['21'] = combined_data['20:15'] + combined_data['20:30'] + combined_data['20:45'] + combined_data['21:00']\n",
    "combined_data['22'] = combined_data['21:15'] + combined_data['21:30'] + combined_data['21:45'] + combined_data['22:00']\n",
    "combined_data['23'] = combined_data['22:15'] + combined_data['22:30'] + combined_data['22:45'] + combined_data['23:00']\n",
    "combined_data['0'] = combined_data['23:15'] + combined_data['23:30'] + combined_data['23:45'] + combined_data['0:00']\n",
    "combined_data['2 (DST)'] = combined_data['01:15 (DST)'] + combined_data['01:30 (DST)'] + combined_data['01:45 (DST)'] + combined_data['02:00 (DST)']\n",
    "\n",
    "# keep only the fuel and date columns, and the column we just created\n",
    "combined_data = combined_data[['Fuel', 'Date', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
    "                                    '21', '22', '23', '0', '2 (DST)']]\n",
    "\n",
    "# instead of having a column for each hour, we want to have a column for the date and hour and a column for the value\n",
    "# first, we need to melt the data\n",
    "combined_data = pd.melt(combined_data, id_vars=['Fuel', 'Date'], var_name='Hour', value_name='Value')\n",
    "\n",
    "# now we need to combine the date and hour columns\n",
    "combined_data['Date'] = combined_data['Date'].astype(str)\n",
    "combined_data['Hour'] = combined_data['Hour'].astype(str)\n",
    "combined_data['Hour'] = combined_data['Hour'].str.zfill(2)\n",
    "\n",
    "# we need to convert the hour column to datetime\n",
    "\n",
    "combined_data['DateHour'] = combined_data['Date'] + ' ' + combined_data['Hour']\n",
    "\n",
    "# now we can drop the date and hour columns\n",
    "combined_data = combined_data.drop(columns=['Hour'])\n",
    "combined_data = combined_data.dropna(subset=['Value'])\n",
    "\n",
    "# save the row with the (DST) value in a separate dataframe and remove it from the main dataframe\n",
    "dst_data = combined_data[combined_data['DateHour'].str.contains('DST')]\n",
    "combined_data = combined_data[~combined_data['DateHour'].str.contains('DST')]\n",
    "\n",
    "# remove the (DST) from the DateHour column\n",
    "dst_data['DateHour'] = dst_data['DateHour'].str.replace(' (DST)', '')\n",
    "\n",
    "# now we can convert the DateHour column to a datetime\n",
    "combined_data['DateHour'] = pd.to_datetime(combined_data['DateHour'], format='%Y-%m-%d %H')\n",
    "dst_data['DateHour'] = pd.to_datetime(dst_data['DateHour'], format='%Y-%m-%d %H')\n",
    "\n",
    "# Pivot the DataFrame to have 'Fuel' as columns\n",
    "combined_data = combined_data.pivot_table(index=['DateHour', 'Date'], columns=['Fuel'], values=['Value']).reset_index()\n",
    "dst_data = dst_data.pivot_table(index=['DateHour', 'Date'], columns=['Fuel'], values=['Value']).reset_index()\n",
    "\n",
    "# add the (DST) data back to the main dataframe using concat\n",
    "combined_data = pd.concat([combined_data, dst_data])\n",
    "\n",
    "# Create a custom sorting key to sort the 'DateHour' column\n",
    "def custom_sort_key(x):\n",
    "    hour = x.hour\n",
    "    if hour == 0:\n",
    "        hour = 24  # Assign a value greater than other hours to make it appear last\n",
    "    return hour\n",
    "\n",
    "combined_data['SortKey'] = combined_data['DateHour'].apply(custom_sort_key)\n",
    "\n",
    "# Sort the DataFrame by the custom sorting key\n",
    "combined_data = combined_data.sort_values(by=['Date', 'SortKey'])\n",
    "\n",
    "# Remove the 'SortKey' and 'Date' column\n",
    "combined_data = combined_data.drop(columns=['SortKey', 'Date'])\n",
    "\n",
    "# Keep only the fuel names in the column names\n",
    "combined_data.columns = combined_data.columns.droplevel(0)\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "combined_data = combined_data.reset_index(drop=True)\n",
    "\n",
    "print(combined_data.head(10))\n",
    "\n",
    "# Save the combined data to a CSV file  \n",
    "combined_data.to_csv('IntGenbyFuel2021_hourly.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
